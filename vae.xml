<?xml version="1.0" encoding="UTF-8"?>
<problems>
  <problem>
    <question>What is the main purpose of the reparameterization trick in VAEs?</question>
    <answers>
      <answer>To reduce the computational complexity of the encoder network</answer>
      <answer>To increase the dimensionality of the latent space</answer>
      <answer correct="true">To make the sampling process differentiable so gradients can be computed</answer>
      <answer>To enforce the prior distribution to be Gaussian</answer>
    </answers>
    <explanation>The reparameterization trick is used to make the stochastic sampling operation differentiable. Instead of sampling $z$ directly from $N(\mu, \sigma^2)$, we reparameterize it as $z = \mu + \sigma \epsilon$ where $\epsilon \sim N(0, I)$. This separates the stochastic component from the parameters, allowing gradients to flow through the network during backpropagation.</explanation>
  </problem>
</problems> 