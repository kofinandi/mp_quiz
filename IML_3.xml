<?xml version="1.0" encoding="UTF-8"?>
<problems>
  <problem>
    <question>According to the definition provided, what is Human-Centered Machine Learning (HCML)?</question>
    <answers>
      <answer>A field of research that prioritizes machine performance over human understanding in co-adaptive machine learning scenarios.</answer>
      <answer correct="true">A field of research that considers humans and machines as equally important actors in the design, training, and evaluation of co-adaptive machine learning scenarios.</answer>
      <answer>A field of research focused solely on the design of user interfaces for machine learning systems, without considering training or evaluation.</answer>
      <answer>A field of research that aims to replace human actors with fully autonomous machine learning systems in all scenarios.</answer>
    </answers>
    <explanation>Slide 9 of "Human-Centered Evaluation" defines Human-Centered Machine Learning (HCML) as "a field of research that considers humans and machines as equally important actors in the design, training, and evaluation of co-adaptive machine learning scenarios."</explanation>
  </problem>
  <problem>
    <question>Which of the following best describes the "Visual Information Seeking Mantra" proposed by Ben Schneiderman?</question>
    <answers>
      <answer>Details first, then zoom and filter, overview last.</answer>
      <answer>Filter aggressively, overview rarely, details on demand.</answer>
      <answer correct="true">Overview first, zoom and filter, details on demand.</answer>
      <answer>Zoom continuously, filter selectively, overview as needed.</answer>
    </answers>
    <explanation>Slide 11 of "Interaction Design" presents Ben Schneiderman's Visual Information Seeking Mantra as: "Overview first, zoom and filter, details on demand."</explanation>
  </problem>
  <problem>
    <question>In the context of user-centered design, what is the "Gulf of Execution"?</question>
    <answers>
      <answer>The effort required by the user to interpret the system's state and determine if their goals were met.</answer>
      <answer correct="true">The difference between the user’s intentions and the allowable actions of the system.</answer>
      <answer>The user's understanding of how the system internally processes data.</answer>
      <answer>The system's ability to provide clear feedback to the user after an action is performed.</answer>
    </answers>
    <explanation>Slide 20 of "Interaction Design" defines the Gulf of Execution as "The difference between the user’s intentions and the allowable actions." It determines if users understand where to perform actions.</explanation>
  </problem>
  <problem>
    <question>Which basic interaction type involves showing the user a different representation of the data, potentially changing the type of visual encoding used?</question>
    <answers>
      <answer>Reconfigure</answer>
      <answer>Filter</answer>
      <answer correct="true">Encode</answer>
      <answer>Abstract/Elaborate</answer>
    </answers>
    <explanation>Slide 26 of "Interaction Design" describes "Encode" as the interaction type: "show me a different representation [different type]". This implies changing how the data is visually represented.</explanation>
  </problem>
  <problem>
    <question>What is a primary characteristic of "Explainability" in AI models, as discussed in the lectures?</question>
    <answers>
      <answer>The explanation must maximize the model's predictive accuracy above all else.</answer>
      <answer>The explanation must be generated by a model that is inherently simple, like a linear model.</answer>
      <answer correct="true">The explanation must be faithful to the model's behavior and comprehensible for the target audience.</answer>
      <answer>The explanation must provide a complete mathematical derivation of the model's decision process.</answer>
    </answers>
    <explanation>Slide 3 of "Interactive &amp; Explainable Machine Learning Applications" (and slide 10 of "Explainability Foundations") states that explainability involves revealing decision-making processes such that "The explanation is faithful (representative) to the model behavior" and "The explanation is comprehensible for the target audience."</explanation>
  </problem>
  <problem>
    <question>Which narrative design structure is characterized by starting with a broad overview, narrowing down to specific details, and then broadening out again to a conclusion?</question>
    <answers>
      <answer>Martini Glass</answer>
      <answer>Inverted Pyramid</answer>
      <answer correct="true">Hourglass</answer>
      <answer>Diamond</answer>
    </answers>
    <explanation>Slide 40 of "Interactive &amp; Explainable Machine Learning Applications" illustrates the "Hourglass" narrative structure, which typically starts broad (Zoom Out), focuses in on specifics (Zoom In), and then broadens out again (Zoom Out).</explanation>
  </problem>
  <problem>
    <question>According to the lectures, what is a "Clever Hans" predictor in the context of machine learning?</question>
    <answers>
      <answer>A predictor that is exceptionally intelligent and outperforms all benchmarks.</answer>
      <answer>A predictor that relies on complex, high-dimensional features that are difficult for humans to understand.</answer>
      <answer correct="true">A predictor that learns unintended shortcut patterns in the data, which may not generalize well, rather than the true underlying concept.</answer>
      <answer>A predictor that is designed specifically for animal-related image classification tasks.</answer>
    </answers>
    <explanation>Slide 79 of "Explainability Foundations" discusses "Unmasking Clever Hans Predictors" in the context of "Shortcut Learning." This refers to models that learn spurious correlations or shortcuts in the training data instead of the intended concept, much like the horse Clever Hans appeared to do math but was actually reacting to cues.</explanation>
  </problem>
  <problem>
    <question>In the explAIner framework, which of the following is NOT listed as one of the core tasks an XAI pipeline might support?</question>
    <answers>
      <answer>Understanding</answer>
      <answer>Diagnosis</answer>
      <answer correct="true">Deployment</answer>
      <answer>Refinement</answer>
    </answers>
    <explanation>Slides 29-31 of "Explainability Foundations" depict the XAI Pipeline within the explAIner framework as supporting the tasks of Understanding, Diagnosis, and Refinement. Deployment, while part of the overall ML lifecycle (slide 14), is not listed as one of these core XAI tasks within the explAIner visualization.</explanation>
  </problem>
  <problem>
    <question>What is the primary goal of LIME (Local Interpretable Model-Agnostic Explanations)?</question>
    <answers>
      <answer>To provide a global explanation for the entire behavior of a complex model.</answer>
      <answer>To create a simpler, inherently interpretable model that replaces the original black-box model.</answer>
      <answer correct="true">To explain the prediction of any classifier or regressor in an interpretable and faithful manner by learning an interpretable model locally around the prediction.</answer>
      <answer>To primarily visualize the activation patterns of neurons within a deep learning model.</answer>
    </answers>
    <explanation>Slide 61 of "Explainability Foundations" shows LIME, which stands for Local Interpretable Model-Agnostic Explanations. Its core idea is to approximate a black-box model's behavior in the vicinity of a specific instance with a simpler, interpretable model, thus explaining that particular prediction.</explanation>
  </problem>
  <problem>
    <question>What does "Feature Visualization" in the context of XAI primarily aim to achieve, as shown in the Distill.pub examples?</question>
    <answers>
      <answer>To visualize the overall architecture of the neural network.</answer>
      <answer>To create a simplified version of the input data that is easier for the model to process.</answer>
      <answer correct="true">To generate examples (e.g., images) that maximally activate a particular neuron or part of a network, showing what the network (or part) is looking for.</answer>
      <answer>To plot the distribution of features in the training dataset.</answer>
    </answers>
    <explanation>Slides 67-72 of "Explainability Foundations" (referencing Distill.pub) explain that feature visualization answers questions about what a network (or parts of a network) are looking for by generating examples, often by optimizing an input (like random noise) to activate a particular neuron or layer.</explanation>
  </problem>
  <problem>
    <question>In the survey on Human-Centered Evaluations of HCML, what was a common finding regarding participant demographics reporting?</question>
    <answers>
      <answer>Nearly all studies reported detailed participant age ranges and gender distributions.</answer>
      <answer>Most studies focused on participants with high ML/AI expertise and low domain expertise.</answer>
      <answer correct="true">While all surveyed studies reported the total number of participants, only a minority reported participant age ranges or gender distribution.</answer>
      <answer>The average number of participants in surveyed evaluations was over 50, ensuring robust statistical power.</answer>
    </answers>
    <explanation>Slide 28 of "Human-Centered Evaluation" states: "All surveyed studies reported the total number of participants", but "20% of studies reported participant age ranges" and "31% of the total studies reported gender distribution." This indicates that while total numbers were reported, detailed demographics were often missing.</explanation>
  </problem>
  <problem>
    <question>Which of the following best describes a "Multi-Stage Pair Analytics Study" as outlined in the lecture?</question>
    <answers>
      <answer>A fully automated evaluation method using two different AI models to analyze data without human intervention.</answer>
      <answer>A quick, single-session usability test focused solely on quantitative metrics collected via standardized questionnaires.</answer>
      <answer correct="true">A structured, typically qualitative study involving stages like expectation checks, approach introduction, interactive sessions, concluding interviews, and questionnaires, often with logging and transcription.</answer>
      <answer>An evaluation that pairs participants to compete against each other in using a machine learning system, with the winner's feedback being prioritized.</answer>
    </answers>
    <explanation>Slides 54-59 of "Human-Centered Evaluation" detail the typical structure of a "Multi-Stage Pair Analytics Study," which includes several stages like Welcome, Semi-Structured Interview for Expectation Check, Approach Introduction, Interactive Analytics Session, Concluding Semi-Structured Interview, and Standardized Questionnaire. It emphasizes logging performance data, interaction sequences, and audio/video recording.</explanation>
  </problem>
  <problem>
    <question>What is one of the key challenges highlighted regarding "Human Centered Evaluations" in machine learning?</question>
    <answers>
      <answer>Human-centered evaluations are generally faster and cheaper to conduct than algorithm-centered evaluations.</answer>
      <answer correct="true">There is often a lack of ground truth, especially for exploratory or personalized analysis results, making it difficult to objectively measure success.</answer>
      <answer>Human participants typically provide more consistent and less biased feedback than automated metrics.</answer>
      <answer>Most human-centered evaluations involve very large numbers of participants, making data analysis complex.</answer>
    </answers>
    <explanation>Slide 51 of "Human-Centered Evaluation" lists "Lack of Ground Truth" as a challenge, noting that "Exploratory or personalized analysis results" and "Multi-stage evaluations pass to further participants for evaluation or ranking" contribute to this issue.</explanation>
  </problem>
  <problem>
    <question>In the context of "Teachable Machines," what are the three main steps involved in creating a model?</question>
    <answers>
      <answer>Code, Compile, Deploy</answer>
      <answer>Visualize, Interact, Explain</answer>
      <answer correct="true">Gather, Train, Export</answer>
      <answer>Input, Process, Output</answer>
    </answers>
    <explanation>Slide 96 of "Interaction Design" (which is slide 100 in the combined numbering if it were one deck) shows the Teachable Machines workflow as: 1. Gather (gather and group your examples), 2. Train (train your model), and 3. Export (export your model).</explanation>
  </problem>
  <problem>
    <question>What is a "rationalization trap" in the context of language model explainability, as mentioned in the lectures?</question>
    <answers>
      <answer>When a language model provides an overly complex explanation for a simple decision.</answer>
      <answer>When users over-trust the explanations provided by a language model due to its human-like output.</answer>
      <answer correct="true">When the explanation provided by a system (e.g., for a language model) aligns with a user's mental model of language but diverges from the model's actual reasoning, leading to an untruthful understanding of model behavior.</answer>
      <answer>When a language model hallucinates an explanation that is completely unrelated to the input query.</answer>
    </answers>
    <explanation>Slide 17 of "Interactive &amp; Explainable Machine Learning Applications" discusses "Building Appropriate Mental Models" and references "Sevastjanova R, El-Assady M. Beware the rationalization trap! when language model explainability diverges from our mental models of language, 2022." The diagram illustrates how a user might accept an explanation that fits their mental model (truthful or untruthful rationalization) even if the model's behavior is different, particularly if the explanation has lower fidelity to the actual model workings.</explanation>
  </problem>
  <problem>
    <question>What is the primary difference between Algorithm-Centered Evaluation and Human-Centered Evaluation in the context of Interactive and Explainable ML?</question>
    <answers>
      <answer>Algorithm-Centered Evaluation focuses on user satisfaction, while Human-Centered Evaluation focuses on model accuracy.</answer>
      <answer correct="true">Algorithm-Centered Evaluation focuses on how the model decided (e.g., metrics like precision, recall), while Human-Centered Evaluation focuses on how the model's decision aligns with the user's understanding and interaction (e.g., trust, usability).</answer>
      <answer>Algorithm-Centered Evaluation uses qualitative methods, while Human-Centered Evaluation uses quantitative methods.</answer>
      <answer>There is no significant difference; both evaluate the same aspects of the ML system.</answer>
    </answers>
    <explanation>Slide 10 of "Human-Centered Evaluation" contrasts Algorithm-Centered Evaluation ("This is how the model decided!") with Human-Centered Evaluation ("This is how the model’s decision aligns with the user’s understanding!"). The former uses metrics like precision/recall, while the latter uses user studies, crowdsourcing, etc., to assess alignment with user understanding, trust, and usability.</explanation>
  </problem>
  <problem>
    <question>Which of these is NOT a typical interaction modality beyond mouse and keyboard, as discussed in the "Interaction Design" lecture?</question>
    <answers>
      <answer>Physicalization (Physical Visualizations)</answer>
      <answer>Gestural Interactions</answer>
      <answer>Eye Tracking</answer>
      <answer correct="true">Command Line Interface (CLI)</answer>
    </answers>
    <explanation>Slides 13-17 of "Interaction Design" discuss "Interaction Modalities beyond Mouse and Keyboard," including Physicalization, Gestural Interactions, Eye Tracking, and Device-based Interaction (like active ink). A Command Line Interface is a traditional text-based interaction, not typically categorized as a modality "beyond" mouse and keyboard in this context.</explanation>
  </problem>
  <problem>
    <question>What is the "Gulf of Evaluation" in user-centered design?</question>
    <answers>
      <answer>The user's difficulty in finding where to perform an action in the interface.</answer>
      <answer correct="true">The amount of effort that the person must exert to interpret the state of the system and to determine how well the expectations and intentions have been met.</answer>
      <answer>The gap between the system's internal model and the user's mental model.</answer>
      <answer>The system's inability to provide diverse options for user interaction.</answer>
    </answers>
    <explanation>Slide 20 of "Interaction Design" defines the Gulf of Evaluation as "The amount of effort that the person must exert to interpret the state of the system and to determine how well the expectations and intentions have been met." It communicates the response to the users' actions.</explanation>
  </problem>
  <problem>
    <question>When designing interactions, "Affordances" refer to:</question>
    <answers>
      <answer>The aesthetic appeal of the interface elements.</answer>
      <answer>The signals or clues that tell a user where to perform an action.</answer>
      <answer correct="true">The perceived possible actions that a user can take with an object or interface element.</answer>
      <answer>The efficiency with which a user can complete a task.</answer>
    </answers>
    <explanation>Slide 22 of "Interaction Design" defines Affordances as "= Perceived Action," indicating what actions are perceived to be possible with an interface element. Signifiers, in contrast, indicate where to perform an action.</explanation>
  </problem>
  <problem>
    <question>Which cognitive bias describes the tendency for users to rely heavily on the first piece of information they see?</question>
    <answers>
      <answer>Availability Heuristic</answer>
      <answer correct="true">Anchoring</answer>
      <answer>Representativeness Heuristic</answer>
      <answer>Status Quo Bias</answer>
    </answers>
    <explanation>Slide 23 of "Interaction Design" defines "Anchoring" as the bias where "Users rely heavily on the first piece of information they see."</explanation>
  </problem>
  <problem>
    <question>What is the primary purpose of "Semantic Zoom" as a direct interaction technique?</question>
    <answers>
      <answer>To change the color scheme of the visualization based on zoom level.</answer>
      <answer>To simply magnify a portion of the visualization without changing the representation of elements.</answer>
      <answer correct="true">To change the visual representation of data items as the zoom level changes, revealing more or less detail or different types of information.</answer>
      <answer>To link the zoom level to an external dataset for filtering.</answer>
    </answers>
    <explanation>Slide 60 of "Interaction Design" illustrates "Direct Interaction: Semantic Zoom." Semantic zoom implies that as the user zooms in or out, the way data is represented changes to be appropriate for that level of detail, not just a simple magnification.</explanation>
  </problem>
  <problem>
    <question>In the context of XAI, "Adversarial Testing" primarily aims to evaluate:</question>
    <answers>
      <answer>How well a model's explanations align with human intuition.</answer>
      <answer>The speed and efficiency of the explanation generation process.</answer>
      <answer correct="true">The robustness of a model to small, often imperceptible, perturbations in the input data designed to cause misclassification.</answer>
      <answer>The interpretability of the model's internal architecture.</answer>
    </answers>
    <explanation>Slide 55 of "Explainability Foundations" describes Adversarial Testing (or Sensitivity Analysis) as questioning "How robust is the model given small perturbations?" and "What is the most minimal change to be introduced, leading to a different decision?". Slide 56 further illustrates adversarial attacks.</explanation>
  </problem>
  <problem>
    <question>What is a key characteristic of "human explanations" as discussed in the lecture on explainability applications?</question>
    <answers>
      <answer>They are always exhaustive and cover every possible detail.</answer>
      <answer>They are rigidly formal and avoid social context.</answer>
      <answer correct="true">They are often contrastive, selective, social, and adaptive.</answer>
      <answer>They are static and do not change based on the audience.</answer>
    </answers>
    <explanation>Slide 15 of "Interactive &amp; Explainable Machine Learning Applications" lists characteristics of human explanations as being contrastive, selective, social, and adaptive.</explanation>
  </problem>
  <problem>
    <question>What are "Explorables" in the context of communicating ML/XAI concepts?</question>
    <answers>
      <answer>Formal research papers detailing new XAI algorithms.</answer>
      <answer>Static dashboards presenting model performance metrics.</answer>
      <answer correct="true">Interactive articles or systems designed to help users explore and understand complex concepts (like ML models or XAI techniques) in an engaging way, often with a narrative or guided discovery.</answer>
      <answer>Proprietary software tools used by ML experts for model debugging.</answer>
    </answers>
    <explanation>Slides 52-57 of "Interactive &amp; Explainable Machine Learning Applications" discuss "Explorables" and provide examples like xai-primer.com, explorabl.es, and Distill.pub articles, all of which are interactive mediums for explaining complex topics.</explanation>
  </problem>
  <problem>
    <question>What is Layer-Wise Relevance Propagation (LRP) primarily used for?</question>
    <answers>
      <answer>To simplify the architecture of a neural network by removing irrelevant layers.</answer>
      <answer>To generate counterfactual examples by propagating changes backward through the network.</answer>
      <answer correct="true">To attribute the prediction of a neural network to its input features by propagating relevance scores backward through the network layers.</answer>
      <answer>To visualize the activation strength of each layer in a neural network during forward propagation.</answer>
    </answers>
    <explanation>Slide 64 of "Explainability Foundations" shows an example of Layer-Wise Relevance Propagation (LRP) as an attribution method that explains a model's output by distributing the prediction "relevance" back to the input features through the layers of the network.</explanation>
  </problem>
  <problem>
    <question>In the explAIner framework, what does the "level (data coverage)" property of an explainer refer to?</question>
    <answers>
      <answer>The number of different machine learning models the explainer can handle.</answer>
      <answer correct="true">Whether the explanation applies to the entire dataset (global) or a specific subset/instance (local).</answer>
      <answer>The complexity of the explanation, ranging from simple visualizations to detailed textual reports.</answer>
      <answer>The stage of the machine learning pipeline at which the explanation is generated (e.g., pre-training, post-hoc).</answer>
    </answers>
    <explanation>Slide 39 of "Explainability Foundations" illustrates the "level (data coverage)" property, showing that a global explanation considers all possible inputs and outputs, while a local explanation focuses on a subset or sample.</explanation>
  </problem>
  <problem>
    <question>What is the main purpose of "Activation Atlases" as presented in the Distill.pub work?</question>
    <answers>
      <answer>To map the geographical origins of different neural network architectures.</answer>
      <answer>To provide a step-by-step tutorial on how to build neural networks.</answer>
      <answer correct="true">To provide a global view of neuron activations by systematically mapping out and visualizing what different neurons or combinations of neurons respond to across a dataset.</answer>
      <answer>To compare the performance of different activation functions within a neural network.</answer>
    </answers>
    <explanation>Slides 71, 74, and 75 of "Explainability Foundations" discuss Activation Atlases from Distill.pub. They show how these atlases provide a comprehensive map of neuron activations, allowing inspection of what features or concepts different parts of the network have learned to detect.</explanation>
  </problem>
  <problem>
    <question>According to the "What is the Narrative Design?" slide, which part of a narrative visualization typically introduces the main characters or concepts and sets the stage?</question>
    <answers>
      <answer>Middle</answer>
      <answer correct="true">Beginning</answer>
      <answer>End</answer>
      <answer>Epilogue</answer>
    </answers>
    <explanation>Slide 4 of "Human-Centered Evaluation" (and slide 39 of "Interactive &amp; Explainable Machine Learning Applications") shows a narrative structure with "Beginning," "Middle," "End," and "Epilogue." The "Beginning" is where the initial context and primary elements are typically introduced.</explanation>
  </problem>
  <problem>
    <question>What is a key characteristic of "Interaction Trails" as a technique for Process, Guidance, and Provenance in interactive systems?</question>
    <answers>
      <answer>They prevent users from making any incorrect interactions.</answer>
      <answer>They automatically generate a final report based on user activity.</answer>
      <answer correct="true">They capture and visualize the sequence of user interactions, providing a history or log of the analytical process.</answer>
      <answer>They guide users through a predefined, unchangeable workflow.</answer>
    </answers>
    <explanation>Slide 65 of "Interaction Design" shows an example of "Interaction Trails," which visually represent the history of user interactions or parameter changes, thus capturing the provenance of the analysis.</explanation>
  </problem>
  <problem>
    <question>The "What-If Tool" by Google PAIR is an example of a technique primarily used for:</question>
    <answers>
      <answer>Generating highly realistic synthetic data.</answer>
      <answer>Automated model deployment and scaling.</answer>
      <answer correct="true">Model probing and understanding model behavior by exploring counterfactuals and comparing model performance across different data slices.</answer>
      <answer>Training large-scale deep learning models on distributed systems.</answer>
    </answers>
    <explanation>Slide 54 of "Explainability Foundations" refers to the "What-If Tool" in the context of "Model Probing," which aligns with its function of exploring counterfactuals and understanding model behavior under different input conditions or for different data subsets.</explanation>
  </problem>
  <problem>
    <question>Which of the following is a primary motivation for using "Interactive Articles" to communicate research or complex topics, as discussed in the lectures?</question>
    <answers>
      <answer>To enforce a strict, linear reading experience without distractions.</answer>
      <answer>To cater exclusively to expert audiences with deep technical knowledge.</answer>
      <answer correct="true">To lower the entry barrier through exploration, connect people with data, and potentially make learning more playful and personalized.</answer>
      <answer>To replace traditional dashboards entirely for all data analysis tasks.</answer>
    </answers>
    <explanation>Slide 29 of "Interactive &amp; Explainable Machine Learning Applications" lists design mechanisms for Interactive Articles, including "Lower entry barrier through exploration," "Connecting people and data," "Making systems playful," and "Personalizing reading."</explanation>
  </problem>
  <problem>
    <question>In the survey of Human-Centered Evaluations for HCML, what was the most common task type given to participants?</question>
    <answers>
      <answer>Justify</answer>
      <answer>Hypothesize</answer>
      <answer correct="true">Understand</answer>
      <answer>Diagnose</answer>
    </answers>
    <explanation>Slide 23 of "Human-Centered Evaluation" presents a bar chart showing the distribution of task types in surveyed studies. "Understand" is the task with the highest number of studies.</explanation>
  </problem>
  <problem>
    <question>What is the "Status Quo Bias" in the context of user interaction and cognitive biases?</question>
    <answers>
      <answer>Users tend to seek out information that confirms their existing beliefs.</answer>
      <answer>Users overestimate the importance of information that is easily recalled.</answer>
      <answer correct="true">Users prefer to stick with default settings or established flows, even if they are not ideal.</answer>
      <answer>Users follow the actions and opinions of a larger group.</answer>
    </answers>
    <explanation>Slide 23 of "Interaction Design" defines "Status Quo Bias" as "Users prefer to stick with default settings or established flows. Example: Most users won’t change default notification settings—even if they’re not ideal."</explanation>
  </problem>
  <problem>
    <question>Which of these is NOT a primary design dimension of the explAIner framework that helps characterize different XAI methods?</question>
    <answers>
      <answer>Number of model states considered</answer>
      <answer>Abstraction (model coverage): high/low</answer>
      <answer>Dependency: data, model, domain</answer>
      <answer correct="true">Computational cost of the explanation</answer>
    </answers>
    <explanation>Slides 37-41 of "Explainability Foundations" detail the explainer properties (design dimensions): (1) number of model states considered, (2) parts of the model state considered, (3) level (data coverage), (4) abstraction (model coverage), and (5) dependency. Computational cost, while a practical consideration, is not listed as one of these core characterizing dimensions in the framework's visualization.</explanation>
  </problem>
  <problem>
    <question>The concept of "Brushing and Linking" in interactive visualization allows users to:</question>
    <answers>
      <answer>Change the visual encoding of all linked views simultaneously.</answer>
      <answer correct="true">Select data points in one view and see the corresponding data points highlighted in other linked views.</answer>
      <answer>Automatically generate a textual summary of the selected data points.</answer>
      <answer>Apply different machine learning models to subsets of data selected via brushing.</answer>
    </answers>
    <explanation>Slide 33 of "Interaction Design" shows an example of "Brushing and Linking," where selecting (brushing) data in one plot causes the same data points to be highlighted (linked) in other plots, facilitating the discovery of relationships across different views of the data.</explanation>
  </problem>
  <problem>
    <question>What does "Occlusion Analysis" as an XAI verification technique typically involve?</question>
    <answers>
      <answer>Adding random noise to the input to see how the model's output changes.</answer>
      <answer>Training a simpler, interpretable model to mimic the behavior of the black-box model.</answer>
      <answer correct="true">Systematically blocking or masking parts of the input (e.g., regions of an image, features) and observing the effect on the model's output or internal states.</answer>
      <answer>Visualizing the gradients of the output with respect to the input features.</answer>
    </answers>
    <explanation>Slide 78 of "Explainability Foundations" illustrates Occlusion Analysis by showing parts of an input image being masked ("Input Occlusion") and a corresponding "Model Occlusion," implying that parts of the model's processing related to those inputs are also affected or analyzed.</explanation>
  </problem>
  <problem>
    <question>In the context of interactive machine learning workflows, what is a key characteristic of a "[Contrastive] Interactive ML Workflow"?</question>
    <answers>
      <answer>A single AI model is iteratively refined by the user through direct feedback.</answer>
      <answer>The user teaches the AI model by providing explicit rules and examples.</answer>
      <answer correct="true">Multiple AI models or agents compete for the user's satisfaction, and evaluation is based on relative comparison rather than absolute judgement.</answer>
      <answer>The AI model explains its decisions to the user without any user interaction.</answer>
    </answers>
    <explanation>Slide 77 of "Interaction Design" describes a "[Contrastive] Interactive ML Workflow" where multiple AI models (e.g., AI Model 1, AI Model 2) are involved, and the text states "Models compete for the user’s satisfaction!" and "Relative comparison instead of absolute judgement!" Slide 79 further reinforces this with "Agents learn personalized user preferences!" through relative comparison.</explanation>
  </problem>
  <problem>
    <question>The "Martini Glass" narrative structure in data storytelling typically features:</question>
    <answers>
      <answer>A broad introduction, a deep dive into specifics, and then a broad conclusion.</answer>
      <answer>A very specific anecdotal opening, followed by general background, and then returning to the anecdote.</answer>
      <answer correct="true">An author-driven, linear introduction presenting key facts (the "stem"), followed by a point where the narrative opens up for more reader-driven exploration (the "bowl").</answer>
      <answer>The most important information presented first, with details of decreasing importance following.</answer>
    </answers>
    <explanation>Slide 41 of "Interactive &amp; Explainable Machine Learning Applications" illustrates the "Martini Glass" structure. It starts with a "Lead + key facts" (author-driven, single direction), followed by "Chronological events," and then opens to an "exploratory" phase (reader-driven).</explanation>
  </problem>
  <problem>
    <question>What is the primary goal of "Provenance Tracking" in visual analytics and interactive machine learning?</question>
    <answers>
      <answer>To predict the user's next interaction based on their current behavior.</answer>
      <answer>To automatically generate explanations for the model's decisions.</answer>
      <answer correct="true">To record and represent the history of user actions, analysis steps, and system states to support review, reproducibility, and understanding of the analytical process.</answer>
      <answer>To guide users towards the most optimal analytical path.</answer>
    </answers>
    <explanation>Slides 66-69 of "Interaction Design" discuss Interaction Log Capture, Provenance Tracking, and Provenance Information Presentation. The core idea is to capture the history of interactions and analytical steps, such as in the `trrack` examples or Tableau History Panel, to understand how a result was obtained.</explanation>
  </problem>
  <problem>
    <question>What is the role of "User Guidance" in interactive systems, particularly in the context of Visual Analytics?</question>
    <answers>
      <answer>To restrict user actions to a single, predefined optimal path.</answer>
      <answer>To replace the need for user expertise by automating all analytical decisions.</answer>
      <answer correct="true">To assist users in navigating complex systems and data by providing support at different stages, such as onboarding, visual mapping, analysis interactions, and data exploration.</answer>
      <answer>To primarily focus on providing entertaining pop-up tips unrelated to the task at hand.</answer>
    </answers>
    <explanation>Slide 71 of "Interaction Design" shows a "User Guidance" framework with levels like Onboarding (System Functionality), Visual Mapping, Analysis Interactions, and Data Exploration, indicating that guidance supports users across various aspects of system use and analysis complexity. Slide 72 further details characterizing guidance in visual analytics.</explanation>
  </problem>
  <problem>
    <question>The "explAIner" framework is designed to help users understand, diagnose, and refine machine learning models by:</question>
    <answers>
      <answer>Automatically selecting the best XAI method for any given model and dataset.</answer>
      <answer>Providing a single, static explanation for a model's overall behavior.</answer>
      <answer correct="true">Structuring XAI as an iterative process, where different XAI methods can be applied to various model states, and their outputs (explanations, transition functions) support different analytical tasks.</answer>
      <answer>Focusing exclusively on inherently interpretable models like decision trees.</answer>
    </answers>
    <explanation>Slides 19 and 36 of "Explainability Foundations" describe the explAIner framework. Slide 36 shows the iterative nature: "Model State (4) abstraction... (1) number of model states considered...". The framework allows applying XAI methods (Explainers) to produce explanations and transition functions, facilitating tasks like Understanding, Diagnosis, and Refinement in an iterative XAI Pipeline.</explanation>
  </problem>
  <problem>
    <question>In the context of interaction design, what does "Discoverability" primarily refer to?</question>
    <answers>
      <answer>How quickly a user can find the system through a search engine.</answer>
      <answer>The aesthetic appeal of the interface elements.</answer>
      <answer correct="true">The ease with which users can find and understand what actions are possible and where to perform them within an interface.</answer>
      <answer>The number of features a system offers.</answer>
    </answers>
    <explanation>Slide 20 of "Interaction Design" links Discoverability to the "Gulf of Execution," which is "The difference between the user’s intentions and the allowable actions." The arrow indicates it "Determines if users understand where to perform actions," which is central to discoverability.</explanation>
  </problem>
  <problem>
    <question>What is a common characteristic of "Dashboard Design Patterns" as presented in the lecture?</question>
    <answers>
      <answer>They prioritize text-based summaries over visual representations.</answer>
      <answer>They are primarily designed for single, non-interactive data displays.</answer>
      <answer correct="true">They offer structured ways to organize data, visual representations, interactions, and page layouts to effectively communicate information at a glance.</answer>
      <answer>They focus on using as many different chart types as possible on a single screen.</answer>
    </answers>
    <explanation>Slide 48 of "Interaction Design" (and slide 44 of "Interactive &amp; Explainable Machine Learning Applications") presents a "Dashboard Design Cheatsheet" covering aspects like Data, Structure, Visual Representation, Page Layout, Screenspace, Interaction, Meta Data, and Color. These patterns help in designing effective dashboards for information communication.</explanation>
  </problem>
  <problem>
    <question>According to the survey on Human-Centered Evaluations, which of the following is a challenge related to "Participants"?</question>
    <answers>
      <answer>Participants are usually too experienced with ML, biasing the results.</answer>
      <answer>Studies consistently recruit a very large and diverse set of participants.</answer>
      <answer correct="true">The average number of participants in surveyed evaluations is often small, and personal characteristics/experiences are rarely considered.</answer>
      <answer>Participants find it difficult to understand algorithm-centered metrics.</answer>
    </answers>
    <explanation>Slide 51 of "Human-Centered Evaluation" lists challenges related to "Participants," including: "On average, six participants in the surveyed evaluations," "Simpler replacement tasks for evaluation at scale are not always a good fit," and "Personal characteristics and experiences are rarely considered."</explanation>
  </problem>
  <problem>
    <question>What is a primary characteristic of a "Deductive" Interactive ML Workflow?</question>
    <answers>
      <answer>The AI model teaches the user how to perform a task.</answer>
      <answer>Multiple AI models compete, and the user selects the best one.</answer>
      <answer correct="true">The user teaches the AI (e.g., by providing labeled examples or feedback), and the AI learns from the user's input to improve its model.</answer>
      <answer>The system deduces user intent purely from passive observation of their unrelated activities.</answer>
    </answers>
    <explanation>Slide 81 of "Interaction Design" illustrates a "[Deductive] Interactive ML Workflow" where the "User teaches AI!" and "AI learns from User!". This implies a flow where user input directly shapes the AI model's learning process.</explanation>
  </problem>
</problems>